---
sidebar: sidebar 
permalink: upgrade-arl-auto/relocate_non_root_aggr_nas_lifs_from_node2_to_node3.html 
keywords: relocate, non-root, aggregates, nas, lif, node2, node3 
summary: 'Spostare gli aggregati non root di node2 in node3 quando si aggiornano i controller che eseguono ONTAP 9.5 a 9.7 utilizzando `system controller replace` comandi.' 
---
= Spostare aggregati non root e LIF dati NAS da node2 a node3
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Prima di sostituire il node2 con node4, spostare gli aggregati non root e le LIF dati NAS di proprietà di node2 in node3.

.Prima di iniziare
Una volta completati i controlli successivi alla fase precedente, la release di risorse per node2 si avvia automaticamente. Gli aggregati non root e le LIF di dati non SAN vengono migrati da node2 a node3.

.A proposito di questa attività
Le LIF remote gestiscono il traffico verso le LUN SAN durante la procedura di aggiornamento. Lo spostamento delle LIF SAN non è necessario per lo stato del cluster o del servizio durante l'aggiornamento.

Una volta migrati gli aggregati e i LIF, l'operazione viene sospesa per scopi di verifica. In questa fase, è necessario verificare se tutti gli aggregati non root e le LIF di dati non SAN vengono migrati in node3.


NOTE: Il proprietario dell'abitazione per gli aggregati e le LIF non viene modificato; solo il proprietario corrente viene modificato.

.Fasi
. Verificare che tutti gli aggregati non root siano online e che il loro stato sia su node3:
+
`storage aggregate show -node _node3_ -state online -root false`

+
L'esempio seguente mostra che gli aggregati non root su node2 sono online:

+
....
cluster::> storage aggregate show -node node3 state online -root false

Aggregate      Size         Available   Used%   State   #Vols  Nodes  RAID     Status
----------     ---------    ---------   ------  -----   -----  ------ -------  ------
aggr_1         744.9GB      744.8GB      0%     online  5      node2  raid_dp  normal
aggr_2         825.0GB      825.0GB      0%     online  1      node2  raid_dp  normal
2 entries were displayed.
....
+
Se gli aggregati sono andati offline o diventano estranei sul node3, portarli online utilizzando il seguente comando sul node3, una volta per ogni aggregato:

+
`storage aggregate online -aggregate _aggr_name_`

. Verificare che tutti i volumi siano online sul nodo 3 utilizzando il seguente comando sul nodo 3 ed esaminando l'output:
+
`volume show -node _node3_ -state offline`

+
Se alcuni volumi sono offline sul node3, portarli online utilizzando il seguente comando sul node3, una volta per ogni volume:

+
`volume online -vserver _vserver_name_ -volume _volume_name_`

+
Il `_vserver_name_` da utilizzare con questo comando si trova nell'output del precedente `volume show` comando.

. Verificare che le LIF siano state spostate nelle porte corrette e che lo stato sia `up`. Se le LIF non sono attive, impostare lo stato amministrativo delle LIF su `up` Immettendo il seguente comando, una volta per ogni LIF:
+
`network interface modify -vserver _vserver_name_ -lif _LIF_name_ -home-node _node_name_ -status-admin up`

. Se le porte che attualmente ospitano i file LIF dei dati non esistono sul nuovo hardware, rimuoverle dal dominio di trasmissione:
+
`network port broadcast-domain remove-ports`



. [[step5]]verificare che non vi siano LIF di dati rimasti sul node2 immettendo il seguente comando ed esaminando l'output:
+
`network interface show -curr-node _node2_ -role data`

. Se sono stati configurati gruppi di interfacce o VLAN, completare i seguenti passaggi secondari:
+
.. Registrare le informazioni sulla VLAN e sul gruppo di interfacce in modo da poter ricreare le VLAN e i gruppi di interfacce sul nodo 3 dopo l'avvio del nodo 3.
.. Rimuovere le VLAN dai gruppi di interfacce:
+
`network port vlan delete -node _nodename_ -port _ifgrp_ -vlan-id _VLAN_ID_`

.. Verificare se sono presenti gruppi di interfacce configurati sul nodo immettendo il seguente comando ed esaminandone l'output:
+
`network port ifgrp show -node _node2_ -ifgrp _ifgrp_name_ -instance`

+
Il sistema visualizza le informazioni sul gruppo di interfacce per il nodo, come illustrato nell'esempio seguente:

+
[listing]
----
cluster::> network port ifgrp show -node node2 -ifgrp a0a -instance
                 Node: node3
 Interface Group Name: a0a
Distribution Function: ip
        Create Policy: multimode_lacp
          MAC Address: 02:a0:98:17:dc:d4
   Port Participation: partial
        Network Ports: e2c, e2d
             Up Ports: e2c
           Down Ports: e2d
----
.. Se nel nodo sono configurati gruppi di interfacce, registrare i nomi di tali gruppi e le porte ad essi assegnate, quindi eliminare le porte immettendo il seguente comando, una volta per ciascuna porta:
+
`network port ifgrp remove-port -node _nodename_ -ifgrp _ifgrp_name_ -port _netport_`




